## Datadog整体介绍
### 发展历史
Datadog成立于2010年，是一家面向开发者、IT运维团队及业务人员的云监控平台公司，致力于为企业客户提供底层系统和上层应用的实时监控、分析能力。从Datadog的发展史，我们可以看出2017年是一个分界线，2017年前致力于ITIM（IT Infrastructure Management，IT基础设施监控）领域，并实现了全球领先；2017年后，业务范围迅速扩展，逐步支持了APM（Application Performance Monitor，应用性能监控）、Log Management（日志管理）、UX（User Experience，用户体验）、NPM（Network Performance Monitoring，网络性能监控）、RUM（Real User Monitoring， 用户体验监控）、Cloud SIEM、性能分析器（Continuous Profiler）等业务领域。
![image.png](/img/src/technical/丝滑的日志接入体验/7e830b7d4f634894d810221b3d7a65e61a27493c192b36af7e53a6cf8232c7f1.png)
### 产品形态
随着产品不断迭代扩展，Datadog 逐步形成一套功能完整的端对端云原生可观测解决方案。

- ITIM 产品能够提供跨部署环境（公共云、私有云和混合环境）的 IT 基础架构的实时、集中监控。
- APM 产品能够跨微服务、主机、容器和Serverless等架构， 提供应用程序运行状况的全面可观测性。
- Log Management 产品能够摄入各类数据，通过创建索引提供日志查询能力，并支持可视化和告警功能。
- Cloud SIEM 可以实时检测和调查所接入数据的安全性，实现实时的威胁检测。
- RUM 可以将可视性提升到技术堆栈上方， 以监控用户端的数字体验。
- NPM 产品可在基于云或混合的环境中分析和可视化网络流量。
- 性能分析器能够分析代码性能，对代码瓶颈进行故障排除和优化。

![image.png](/img/src/technical/丝滑的日志接入体验/7faa7dc1a7d0db371bbadb9e2025a28eadabff17dbe76db49b819210516d96b2.png)
### 产品优势

- 一体化的数据平台：2018 年推出的日志管理产品，Datadog 成为市场上第一家将可观测性的三大数据（log、metric、trace）集一身的厂商。底层数据的打通使得平台上层的 ITIM、APM、NPM 等功能不再孤立，便于进行各种关联分析，以实现更深层次的洞察。
- 产品易用性和高效性：开箱即用，UI 设计流畅，处处体现了自助式交互的理念，新用户易上手，无需过多专业培训即可投入使用。

![image.png](/img/src/technical/丝滑的日志接入体验/05b159d0852090df298a5fb98ea01ae184362d82b21c7232e824d741c2e3bfa6.png)
- 跨云支持：Datadog 云原生的产品能够同时服务于包括公共云、私有云、本地部署和多云混合环境下的 IT 监控需求， 顺应了混合云和多云部署的时代趋势，允许企业保持基础设施的多样性，减少单一厂商依赖。

![image.png](/img/src/technical/丝滑的日志接入体验/6fefe7868197793920a79707e8ff1541a87a019011a6a5ecfe425488aa63a62d.png)

- 强大的探针采集能力：Datadog的核心技术壁垒，因为探针技术牵扯到底层操作系统，实现难度非常大，代码一旦写的不严谨就容易搞宕用户的生产系统。Datadog 的探针历经10年锤炼而成，整体逻辑写的非常严谨且支持框架广泛。
- 丰富的数据源集成：Datadog 的产品通过 Agent 从服务器等硬件或是应用程序中提取数据，用于性能分析。目前，Datadog 平台提供了 500 种内置集成，包括与主流的公有云&私有云、硬件、数据库以及其他监控平台厂商的集成。

![image.png](/img/src/technical/丝滑的日志接入体验/5b1dd3f19fc2c6805654024acedfcac5aaa6edf69ea8e8204dca38b3f6cea7c7.png)
## 日志管理（Log Management）
日志作为可观测数据的三大柱石之一，在可观测体系中起着举足轻重的作用。Datadog 通过收购 Logmatic.io 公司，推出了 Log Management 产品，支持采集或导入日志数据到 Datadog，并提供了丰富的日志管理、分析处理、安全威胁调查等功能。此外， 在同一视图下提供给了 log、metric、trace的统一分析能力，提供了丰富的日志上下文关联能力。
传统日志系统一般会接入所有日志并且进行索引，这往往会造成成本不堪承受（左侧方案）；或者只筛选及索引化一部分日志，就会影响整体可见度（右侧方案）。
![image.png](/img/src/technical/丝滑的日志接入体验/02118b249ec205c921c0740c58fc0fc133825fbe6ca62a8f47aa52db28f48963.png)
而 Datadog 基于 Logging without Limits 的理念，将日志接入与索引剥离，以低成本地摄入全量 log，只针对有分析有价值的 log 进行索引，为客户提供有价值的数据分析功能。
![image.png](/img/src/technical/丝滑的日志接入体验/6f2d5dc4ecba61fdb2c924ef29b3d4d1c9bc0737df0df131a87467d87edb625d.png)
### 产品优势

- Logging without Limits
   - 可以低成本地摄入所有的日志，支持Live tail实时查看日志上下文、异常规则探测；支持基于全量日志生成指标，以便分析日志趋势。
   - 可以只针对有分析有价值的日志进行索引，为客户提供有价值的数据分析功能。也可以根据业务需求，通过 Filter 规则、采样率调整索引策略。
   - 支持基于全量日志进行日志归档，进一步降低存储成本；也可以从压缩的归档文件中恢复日志并在  Datadog 中访问它们，以支持审核或调查。

![image.png](/img/src/technical/丝滑的日志接入体验/8bdb361a732bb2d5d14be48c6a7c7ad9d237a67fdd2d2393db065d739e3284aa.png)

- 快速故障排除和探索能力
   - 快速搜索、过滤和分析日志，以进行故障排除和数据的开放式浏览。
   - 在仪表板上可视化日志数据或构建复杂的警报。
   - Log patterns 提供日志事件的实时分析和聚类能力，从而快速识别异常。
- 统一可观测性分析能力
   - 在日志、指标和跟踪之间顺畅跳转。
   - 使用相同的标签（主机，服务等）将指标图可以直接从指标图转到相关日志。
   - 从任何日志条目跳转到主机指标的仪表板。
   - APM Service 上下文中可以直接查看相关Logs。
- 基于 Processing pipelines 提供统一的日志处理能力。
### 日志接入体验
Datadog数据接入流程简单，只需要几步即可完成最基本的接入。接下来，以Golang接入体验进行介绍。

- Datadog 推荐使用 Json 格式进行日志打印，避免多余的解析开销。
```go
package main

import (
    "os"
    "time"

    "github.com/sirupsen/logrus"
    log "github.com/sirupsen/logrus"
)

func main() {
    logfile, _ := os.OpenFile("./app.log", os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)
    logrus.SetOutput(logfile)

    // use JSONFormatter
    log.SetFormatter(&log.JSONFormatter{})
    for i := 0; i < 1000; i++ {
        // log an event as usual with logrus
        log.WithFields(log.Fields{
            "app": "foo-app",
            "loop": i}).Info("101.228.41.41 - - [10/Aug/2017:14:57:51 +0800] \"POST /PutData?Category=YunOsAccountOpLog\" 0.024 18204 200 37 - aliyun-sdk-go")    
    }
}
```

- 控制台接入，选择Go应用，将会给出详细的配置提示。
   - 日志采集功能默认关闭的，需要在 `datadog.yaml`中设置`logs_enabled: true`。
   - 在`/etc/datadog-agent/conf.d/go.d/conf.yaml`配置日志的采集路径。

![image.png](/img/src/technical/丝滑的日志接入体验/b6d6d91e42b7fb77f9bd7221885e7477bffefac9b9ee1ac5ca3316ea62447efa.png)

- 控制台查看日志接入完成。

![image.png](/img/src/technical/丝滑的日志接入体验/050e1edeaba12a77bc65e68fa1a8d77d252e18b00bfd064d8692791dfb042ed1.png)
### 数据格式
日志作为一种重要的可观测数据，提供了极大的灵活性，不同的业务系统可以根据自己的需求灵活定制，但是后续的分析也带来了碎片化，增加了处理的难度。因此有必要进行一些格式的规范化。
Datadog 在日志规范化方面制定了很多规范。以下是一条典型的 Datadog 日志结构，包括了如下几个部分：

- 日志的级别、时间、Host等属性。
- Tags 信息：支持内置自动探测的Tags(例如主机或者K8s信息)，以及自定义Tags。
- 日志体信息。
- 日志的属性，包括标准属性、自定义属性。

![image.png](/img/src/technical/丝滑的日志接入体验/d2fd623f959f9f6c755457b5f71c25c6af7ec79e0c7259e38ed5c097fb7177c2.png)
同时，为了更好的满足分析需求，也对 attributes（属性） 进行了一些标准化的定义。

- 保留属性 Reserved attributes：Datadog数据接入默认属性，用于标识日志的一些通用信息。
   - host：主机名。
   - source：对应集成名称。
   - status：对应日志的级别。
   - service：对应日志生成的应用或者服务。
   - trace_id
   - message：日志的主体内容。
- 标准属性 Standard attributes ：用于定义一组日志字段的命名约定，datadog预置了[一组标准的属性命名](https://docs.datadoghq.com/logs/log_configuration/attributes_naming_convention/#default-standard-attribute-list)（例如network、http等），便于用户构建自己的日志体系，用户也可以根据自身的情况进行增删。

![image.png](/img/src/technical/丝滑的日志接入体验/b58faf8f64db0ac2b55a0073207d7a0c3cceaab0318dbd5a42bfac250032ffca.png)
虽然建议将日志属性按照上述规范组织，但是对于大规模跨团队协作或者历史原因无法完全按照规范来，这时候在日志分析场景下，可以使用别名进行属性映射。这样就可以在不改变业务系统日志结构的情况下，也能满足关联分析的需求。
![image.png](/img/src/technical/丝滑的日志接入体验/d12206c578c3eb853b1354b290b880b9674c9356bf173a0ddec63d2d7059d4ed.png)
### Pipelines
原始接入的日志往往难以直接满足后续业务分析的需求，Datadog 支持通过 Pipelines 对接入的数据进行处理。Pipeline 内部可以配置多个顺序执行的 Processor，Pipeline 通过 Filter 配置项进行日志的匹配。
Datadog 建议使用Json格式接入，当然也支持通过 Pipelines 可以实现非 Json 格式的处理。
#### Preprocessing
在日志进入 Data Pipeline 处理前，需要进行预处理，将日志内容映射到内置属性（如 timestamp, status, host, service, and message），以便日志能在 Datadog 中具备相同的结构。
![image.png](/img/src/technical/丝滑的日志接入体验/971a3417718dc1475c90751f4b9a6d7522aa13f98b2916eeecc80ca53e9334ae.png)
#### 内置Integration Pipeline

- 对于[integrations数据源](https://docs.datadoghq.com/integrations/#cat-log-collection)，提供了内置 Integration Pipeline 针对特定数据源提供解析能力。内置 Pipeline 是只读的，如需定制修改，可以先 clone 后修改。
- 通过 Integration pipeline library，可以查看所有内置 Integration Pipeline 及配置详情；在这里也可以清晰看到已安装的Pipeline。
- 内置 Integration Pipeline 自动安装，只需要配置相关的source数据源采集即可。一旦 Datadog 接收到对一个 source 的第一条记录，内置 Integration Pipeline 就会自动触发安装，这里也体现了 Datadog 极简的产品设计理念。

![image.png](/img/src/technical/丝滑的日志接入体验/ad4c080e8f7f21150e465125f91e1c928e8936fb261efd9f3b4fc7cd3fd5f281.png)
#### Pipeline管理
可以通过 Pipelines 管理视图查看所有当前生效的 Pipleline信息（包括详细配置、开关等），通过属性的过滤功能可以方便的进行 Pipeline 管理。
![image.png](/img/src/technical/丝滑的日志接入体验/98eaa2cbfd2069bdd9b4df86e8eb6ce5fb1dc030bd006ebe23e3054450ab13fb.png)
Pipeline 作为数据处理管道，如何清晰展示当前运行状态是一个比较重要的因素。Datadog 提供了 Pipeline 的统计指标，用于描述当前管道的数据量信息，并支持详细报表的跳转。
![image.png](/img/src/technical/丝滑的日志接入体验/8f03dd5dd9d6bfb085c08df5774b7c9bd406dcf7520d76779551cf1bbfb1f92d.png)
#### 嵌套 Pipeline 
Pipeline 可以包含嵌套 Pipeline 和 Processor，而嵌套 Pipeline 只能包含 Processor。使用嵌套 Pipeline 将处理进行层次划分。例如下图，先使用第一级 Pipeline 进行前后端划分，之后嵌套 Pipeline 用于子服务的细分。
![image.png](/img/src/technical/丝滑的日志接入体验/441bce40d468d3f5295fbd6754703b052d75e5766955b7e377751a7549bb595e.png)
### Parsing
Datadog自动解析JSON格式的日志；对于其他格式，Grok语法提供了一种比纯正则表达式更简单的解析日志的方法，用于从日志内容中提取attributes。
Datadog Grok语法由三部分组成：%{MATCHER:EXTRACT:FILTER}。

- Matcher：提取规则，内置了number, word, notSpace等。[完整内置列表](https://docs.datadoghq.com/logs/log_configuration/parsing/?tab=matchers#overview)。
- Extract：用于存储Matcher匹配到的文本内容。
- Filter：用于转换匹配结果的post-processor。[完整内置列表](https://docs.datadoghq.com/logs/log_configuration/parsing/?tab=filters#overview)。
#### 使用体验
因为我们知道实际的业务日志往往是复杂多变的，所以在进行字段提取时，往往需要反复多次调试，Grok parser 整体上遵从了“配置即可见”的原则，调测方便、高效。操作步骤如下：

- 点击“Parse My Logs”自动读取采集到的日志，并生成初步的 Grok 解析规则。
- 支持多种日志模式，但是出于效率考虑最多支持5种。
- 日志样例也有一个状态信息（match or no match），用于表示是否有规则匹配。
- 解析规则的任何修改，会实时体现到解析结果中。

![image.png](/img/src/technical/丝滑的日志接入体验/b836bfdbfb83e213c99129c810a7beafdef488dcb507d528bff39a9f910acabb.png)
#### 高级模式

- Datadog默认从 message 字段中使用 Grok Parser 进行字段提取，使用Advanced settings 中 Extract from 可以随意指定提取的字段。
- 对于比较复杂的嵌套规则，可以使用 Helper Rules 定义一些拆分的子规则。

![image.png](/img/src/technical/丝滑的日志接入体验/a996213dfc2fc35f52c286f25737780ded2c320bf96e163addb4880c77401868.png)
#### 内置解析模式
Grok Parser 对于一些常见的日志模式提供了内置的处理能力，简单易用。

- Key value filter：轻松实现key-value格式日志的提取。

![image.png](/img/src/technical/丝滑的日志接入体验/24dce011786870c7c10e58e3535cc28c01b22b89d5ee335cd32c590488b557f3.png)

- 时间解析：使用date matcher配合时间格式定义进行时间提取，精度毫秒。

![image.png](/img/src/technical/丝滑的日志接入体验/5f65ce40b1d2ae0bf6450b05e25749fa778f7f5e3f49ec4bfd1965698e4f917b.png)

- Alternating pattern：通过<REGEX_1>|<REGEX_2>实现规则“或”的选择。
```yaml
# Log:
john connected on 11/08/2017
12345 connected on 11/08/2017

# Rule: 
MyParsingRule (%{integer:user.id}|%{word:user.firstname}) connected on %{date("MM/dd/yyyy"):connect_date}
```

- Optional attribute：某些日志只在部分情况下出现特定的值，通过（）？可以实现可选提取。
```yaml
# Log:
john 1234 connected on 11/08/2017

# Rule:
MyParsingRule %{word:user.firstname} (%{integer:user.id} )?connected on %{date("MM/dd/yyyy"):connect_date}
```

- 嵌套Json
```yaml
# Log:
Sep 06 09:13:38 vagrant program[123]: server.1 {"method":"GET", "status_code":200, "url":"https://app.datadoghq.com/logs/pipelines", "duration":123456}

#Rule:
parsing_rule %{date("MMM dd HH:mm:ss"):timestamp} %{word:vm} %{word:app}\[%{number:logger.thread_id}\]: %{notSpace:server} %{data::json}
```

- Regex：结合正则实现解析能力扩展
```yaml
# Log:
john_1a2b3c4 connected on 11/08/2017

# Rule:
MyParsingRule %{regex("[a-z]*"):user.firstname}_%{regex("[a-zA-Z0-9]*"):user.id} .*
```

- array filter：提取array类型字段。

![image.png](/img/src/technical/丝滑的日志接入体验/4fce49160ee7386cc8644dc21bce4ee290bd677b4f0b704671f7b34cccd19705.png)

- Parsing XML
```yaml
# Log:
<book category="CHILDREN">
  <title lang="en">Harry Potter</title>
  <author>J K. Rowling</author>
  <year>2005</year>
</book>

# Rule:
rule %{data::xml}

# Result:
{
  "book": {
    "year": "2005",
    "author": "J K. Rowling",
    "category": "CHILDREN",
    "title": {
      "lang": "en",
      "value": "Harry Potter"
    }
  }
}
```

- CSV filter
```yaml
# Log:
John,Doe,120,Jefferson St.,Riverside

# Rule:
myParsingRule %{data:user:csv("first_name,name,st_nb,st_name,city")}

# Result:
{
  "user": {
    "first_name": "John",
    "name": "Doe",
    "st_nb": 120,
    "st_name": "Jefferson St.",
    "city": "Riverside"
  }
}
```
### Processors
Datadog 提供了比较丰富的 Processor 进行数据处理。例如，Grok parser用于字段提取；URL parser用于 URL 解析；GeoIP parser用于 IP 地址解析。每个 Processor 的交互逻辑都比较简洁。

- URL parser可以轻松实现URL信息的提取。

![image.png](/img/src/technical/丝滑的日志接入体验/989ab5327d8c569e8f3f3ae9cc645c3caaa0d52c76e68bcab8b2b2d0e5a2c8d8.png)
![image.png](/img/src/technical/丝滑的日志接入体验/eae8424b54cad2a0f009143e8a1d23acc08e355a09f12cfccb94b2d58d35e311.png)

- GeoIP Parser用于获取一个IP地址属性，并提取目标属性路径中的大陆、国家、地区或城市信息。仅需要简单配置字段映射关系即可开启。

![image.png](/img/src/technical/丝滑的日志接入体验/d3aa1afd240f046e453b1ab4fab6655763a780212523be4735d23529d587ef8b.png)
![image.png](/img/src/technical/丝滑的日志接入体验/08ea60d7bf5c91c049e03a7d6616e78d40b7b99f78e71c92f06dd6d2e91e68d6.png)
更多Processor详见[链接](https://docs.datadoghq.com/logs/log_configuration/processors/?tab=ui#overview)。
### Log转Metric
Log-based metrics 提供了基于日志聚合成指标数据的能力，可以有效对原始日志进行降维，进而达到降低存储数据量的目的。
Generate Metric 配置页面基于 Live Tail 进行搜索过滤；支持 Count、Measure 指标类型；也可以通过 Group By 为指标添加聚合的维度。
![image.png](/img/src/technical/丝滑的日志接入体验/0cc50a2633a2716873d5da6fa44609ac55ade705f20a682efc5f74aab813a325.png)
Datadog 提供了内置的 estimated_usage 指标，用于监控流量状态。
![image.png](/img/src/technical/丝滑的日志接入体验/9b62b745e01489987dc840a40fe8416920b37e46ae1f5d1a631698aafed23ee2.png)
### Indexes
在 Datadog 中日志的存储单元是Index，使用 Index 可以将数据划分为不同组，针对TTL、流量配额、使用监控和预算等进行细粒度控制。
![image.png](/img/src/technical/丝滑的日志接入体验/c770138a017d4fd220fac767461bd8fa88ca750fb25adbc067f94155a5329fcd.png)Index 支持通过 Filter 进行选择。当多个 Index 时，数据优先进入第一个匹配的Index。
![image.png](/img/src/technical/丝滑的日志接入体验/90a2ee8315b8a448c01e50b38c83e28c3a4db679a3048de1077869c2f6d98dec.png)
由于不同的日志具有不同的价值，例如测试环境只需要采集Error支持或部分Info日志即可，Index 也可以配置排除过滤器进行采集控制。排除过滤器可以通过查询来圈定范围，设置采样规则进行采样控制，也可以临时关闭打开。
![image.png](/img/src/technical/丝滑的日志接入体验/4cf01cb7cb1f648c34107f6fb640d1008d77892d19f7622c25414ecbf401dbe8.png)
## Observability Pipelines
Vector是一款开源可观测Agent，DataDog于2021年将其收购，之后Datadog增强了很多企业级能力，推出了Observability Pipelines。Observability Pipelines 是一个基于 Vector 的可观测数据处理解决方案，Vector 作为聚合器部署，用于收集、转换和路由所有日志、指标和跟踪到任何目的地。
![image.png](/img/src/technical/丝滑的日志接入体验/fba3f481df360267ffb0fb50d978e3666d0a966a43d29b1f576cedf80cbf68a9.png)
Observability Pipelines一大亮点就是提供了拓扑能力，可以全面了解管道拓扑（接入数据源、处理过程、目的地）并监控关键性能指标，例如每个流的平均负载、错误率和吞吐量。
![image.png](/img/src/technical/丝滑的日志接入体验/8ea875c5b20fc215f4204f1c6d35530d395190d770e3271a9268b434e5d2a773.png)
针对每个 Vector 组件暴露出的运行指标，可以快速识别可观察性数据如何流入管道，以便进行故障排除和性能优化。
![image.png](/img/src/technical/丝滑的日志接入体验/f0e9b360ee35204a2b06c5f19f8341fd8d09b99fc99faee1c9c1cea35481e120.png)
Observability Pipelines 提供了大量开箱即用的[数据源接入和数据输出](https://docs.datadoghq.com/observability_pipelines/integrations/)。此外，输出处理能力也是Vector的一大特色，提供了 [VRL 语法](https://docs.datadoghq.com/observability_pipelines/working_with_data/?tab=yaml)，也是 Vector 的最核心的能力。以Json数据解析为例：
```yaml
# 输入
"{\"status\":200,\"timestamp\":\"2021-03-01T19:19:24.646170Z\",\"message\":\"SUCCESS\",\"username\":\"ub40fan4life\"}"


# 配置
transforms:
  parse_syslog_id:
    type: remap
    inputs:
      - previous_component_id
    source: |2
         . = parse_json!(string!(.message))
         .timestamp = to_unix_timestamp(to_timestamp!(.timestamp))
         del(.username)
         .message = downcase(string!(.message))

# 输出
{
  "message": "success",
  "status": 200,
  "timestamp": 1614626364
}
```
## Log Management 与 Observability Pipelines 的关系
Log Management 是 Datadog 标准的日志方案，是基于datadog agent进行日志采集处理。Observability Pipelines 是基于 Vector 的监控解决方案，目前作为一种集成方式，还没有与 Datadog 日志方案深度集成。
![image.png](/img/src/technical/丝滑的日志接入体验/235bf17d19b85f4828c0e36b642a93a35d76fdcafadc37ac8378550c34800978.png)
两者在功能层面是类似的，都是数据管道机制。Observability Pipelines 提供了更强的拓扑管理及数据处理能力，后续不排除进一步集成的可能。
## 总结
Datadog优秀的用户体验设计为其快速发展打下了坚实的基础。Log Management是 Datadog 用户进行日志接入、查询分析、探索以及故障排查的主要入口，这些功能以交互式的方式提供给用户使用，整体使用流程十分简洁易上手。
## 附录

- [Introducing Logging without Limits™](https://www.datadoghq.com/blog/logging-without-limits)
- [https://docs.datadoghq.com/logs/](https://docs.datadoghq.com/logs/)

> 本文为孙亚宁（寂之）在 2022 TiD 北京站的演讲内容整理。

## 可观测数据使用的痛点
### 可观测数据及常用工具
众所周知，随着业务的发展以及开发模式、系统架构、服务部署模式和基础设施的快速演变，系统会变得越来越复杂，在这个过程中，为了保障系统的稳定性，对于系统的观测就变得越来越重要。区别于传统监控的黑盒模式，可观测性通过各种可观测数据，以白盒的方式对系统进行全方位细致的监测、跟踪与处理。
在这个过程中，我们常用的可观测数据类型主要包括日志、时序指标、Trace，但是这三者并不是完全割裂的。通常情况下，一个系统往往会同时产生这三种数据，我们需要综合结合它们来对系统的稳定性和健康程度做出一个有效的判断。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/857cfb6edaab32b111374618ba772bb49f37e2ed9869fc9cd54a60ab5006602d.png)
> 图片参考 [https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html](https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html)

在实际的工程实践中，为了构建一套可观测平台，我们往往会用到多个组件。例如用 ELK 来采集并存储日志；用各种 exporter 采集指标到 Prometheus，然后进行后续的监控与告警；对于Trace，也有Jaeger、Skywalking、zipkin 等多种方案以及多种存储后端可选。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/bfc0ddedcc78d76a4111a86d19b27a1f11d7e032aaf099882de771ac843dabec.png)
### 可观测数据使用痛点
事实上，抽象来看的话，可观测平台的一般流程，无非就是数据的进与出。在数据采集层通过各种Agent 或者 Exporter，以 Push 或者 Pull 的方式将数据吐出来，然后进行存储与计算，最终可以被监控告警、可视化、机器学习等场景来使用。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/d99b711d1ac2d4a5440b5625f8e0b93f695a033d9adea2a20bf06a208b9960f0.png)
这个流程看起来是很简单直观的，但是在实际的实践过程中，我们往往会遇到各种各样的问题，最常见的包括如下几点：

- **数据格式各异。**比如对于日志来说，有的是相对结构化的文本，有的是很随意的非结构化格式，有的是非常标准的JSON格式，对于使用者来说，比如需要做一些查询分析、报表等场景，不同的数据使用门槛也会有着很大的差异。比如JSON格式的数据，直接就能够通过索引 key 来进行查询分析；但是对于结构混乱的数据，可能需要各种正则匹配以及字符串函数来进行处理后，才能提取出有效的数据进行后续的使用。
- **数据协同困难。**一方面，日志、指标以及Trace可能是存储于不同的系统之中，那么它们之间的协作就有着天然的壁垒，虽然通过二次开发做一些包装可以一定程度上解决该问题，但是还是会有一些开发以及维护的成本。另一方面，我们可能还有一些 CMDB 的数据存储在 MySQL、或者一些存储在对象存储的归档数据，想要将可观测数据与这些数据进行协同，会更加困难。
- **监控复杂度的提升。**比如对于日志和指标的监控方式、配置等方面都存在着比较大的差异，另外就是潜在的告警风暴问题，可能也需要引入额外的组件来进行通知，另外对于一些数据波动比较频繁的场景，传统的监控规则很难动态适应数据的演进趋势，需要不断来调整规则从而适应数据。
- **维护成本高。**首先是不同软件的融合，运维复杂度比较高；然后不同软件的使用方式各异，学习成本也会变高；并且随着数据规模的增长，系统性能、扩展性、稳定性等方面都会面临着很大的挑战。
### 可观测数据治理及使用方案
基于这些痛点，我们使用的是一种相对来说统一的方式来解决。首先在存储层使用统一的可观测存储，来无差别地将日志、指标以及Trace进行存储，并通过统一的查询分析引擎来降低数据的使用门槛和协作门槛。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/d204c1107631098557781e8ed4bdbcb192e7ae80e5994c9c5a4c37b29e4d5481.png)
在计算层，使用数据加工来对非结构化数据进行规整。包括数据的过滤、转化、规整、富化、脱敏等流程，从而降低数据的使用门槛，方便后续场景的使用。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/7b52c1b00e8765389eeb1507f1bf6f515818175c82253c05ddeee743c8233071.png)除了数据加工，还可以通过数据聚集对数据做降维。比如以特定的时间窗口，对数据做聚合与分析，从而将大量的冗余数据转换成少量的低维数据，并且在这个过程中，可以实现日志到日志、日志到指标、以及指标到指标的一个转换过程，从而进一步方便数据的后续使用。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/d4844b5b20e3e9fb4c702d7f7c979f08229a5b9afefe6634c1dd6a785c8437db.png)
经过数据加工和数据聚集处理后，在应用层就可以基于这些数据进行监控的配置，然后进行统一的告警管理与通知管理，从而赋能研发、运维、安全、运营等各个角色。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/1a4c7c53e84dc8d7de6e69b09aa4c0f3a2ac3692334205f94bb11a19eb28f35f.png)
接下来会通过具体的案例来对这些方案进行一个较为详细的介绍。
## 半结构与结构化数据治理
### 数据规整
首先就是数据规整的问题，良好的数据结构在很大程度上可以方便后续监控的使用。例如下面的例子，原始的 nginx-ingress 日志是在一行中，原文采集上来之后，如果想要做分析是比较困难的。但是通过字段解析之后转变成右侧这样的格式，就非常方便使用了。比如就可以直接按照 status 为200 来过滤成功的请求，或则统计不同接口的响应延迟。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/5fcb2cd2f29a5c2cc140545d3304d0039fc460cb66857509f5d062bf2c79ecd6.png)
### 数据富化
另外的一个场景就是数据的富化。比如如图所示的这样一个用户操作日志，日志中展示每个用户什么时候登陆或者做了什么样的数据操作，结果成功还是失败，但是这里记录的只是用户ID，如果我们希望根据用户的其它属性来进行分析，就需要结合用户信息表进行一个信息的整合。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/cb4ef4d9b789383406298e41131ca3f740ba04857ed6d46ce7516440458dc10f.png)
比如将登陆日志与用户信息结合，可以得到富化后的用户登录记录：
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/2740398800a01d26a1469d4f5ec77dd45e5a190987720df54187e1af106549ee.png)
将数据操作日志与用户信息表结合，可以得到富化后的用户操作记录：
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/c717fd9e8330cff34590ca97d31ee96ea13c656cf2094b2848829c69d9d95953.png)
这样再根据用户的其它属性进行分析就会比较方便，例如用户名、用户注册时间、用户级别等。
### 过滤脱敏
还有一个很重要的场景是数据的过滤和脱敏。如图所示是一个原始的通知日志，记录了通知方式、通知接受人信息以及成功或者失败。由于这里记录了通知的原始手机号和邮箱等信息，是相对敏感的一类信息，如果我们要对成功的统计执行进一步的分析，就需要对数据做一次过滤和脱敏，即过滤出成功的日志，并且将敏感信息进行脱敏，从而保证数据使用的安全性。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/5834b5e93556493fab430ec7c4d78b73064d6087a4afa906b1ff76545c714740.png)
### 聚集处理
对于数据量比较大的场景，例如原始的nginx访问日志，每天可能都是数亿条日志。如果要统计最近一个月的访问趋势，那么这么大的数据量无论是从存储成本还是分析性能，都是不太乐观的。因此就需要对数据做定期聚合，例如按照每小时的粒度对数据做聚合，那么原始的千万条日志，可能就会降到几百条甚至更少。这样当我们需要统计某些指标的一段时间内的趋势时，只需要使用聚合后的结果就好了。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/1fab26c5d164f537306788fc462087231dd5586ac054302c43167ab8d7bc7508.png)
## 异构数据协同监控
### 查询分析统一
数据处理好了之后，接下来就是监控场景。我们对标准SQL进行了包装，并结合PromQL 的能力，以及大量的扩展函数，从而可以将日志数据与时序数据的使用变得统一。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/8e0195dea67a5f5d50e3f2eeb48cc259b1a1f7796678f690bc266dab6626758f.png)
### 多数据源集合操作
基于统一的查询分析，可以很容易地实现不同数据之间的协同监控，并进行各种集合操作，比如各种类型的 join、union、exclude 等。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/b5e1e89bcb09b266f91012002766435de28c3dab3312ed6a8476fd0523752457.png)
例如，当业务系统的SLB访问日志出现大量500 错误，并且主机 CPU 使用率超过阈值的时候，触发流量扩容告警。那么就需要同时对访问日志以及主机指标进行监控，并且基于 host 做条件 join，根据这两者共同来判断是否需要最终触发告警。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/43df90a3107bf06a31a2f7890f5e36bd335f36cee1e8edaf79958b7bd56d869d.png)
除了可观测数据之间的协同，在某些情况下，还需要与其它数据进行协同使用，例如 MySQL 中的数据或者对象存储中的 CSV 文件。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/e03d118526c1310fad03bfb1804030ca5184ebee494c2e01c8b70e40cabd0f87.png)
例如有多个服务，每个服务都有各自的 QPS 阈值，如果我们针对这些服务分别配置监控并配置阈值，那么就需要逐个配置，并且后续的维护也会比较麻烦。此时我们可以将服务QPS阈值存储于对象存储的CSV 文件中，然后创建一个虚拟日志表，从而可以让我们的查询引擎直接使用。然后通过 SQL 按照服务名称做聚合，并且与阈值表做 join，在 where 里直接进行实际QPS与阈值的比较，从而可以快速地对所有服务生效。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/9726941b052a9a3537dfbe75d72ee4edc0f3cb653259a054feae7e7c077e14ff.png)
## 智能监控与管理扩展
### 智能巡检
随着云原生的普及，服务部署越来越从以主机为中心向容器化方向转化，容器本身的轻量化以及短生命周期等特点，导致监控对象和监控指标急剧增加。如果要全方位覆盖这些监控对象和指标，需要配置大量的监控规则，并且它们的阈值也可能是各不相同的，因此会有较大的工作量。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/5a483bd6c1940894cd729e0d1fe85546b7245e1b3275a829e5b43d09aa293ae3.png)
另外，基于人为定义的规则阈值，很大程度上依赖于人的经验，随着系统的演化和业务的发展，指标的值可能也会发生变化，此时监控规则就需要人为地进行调整，否则会认为后续的点都是异常情况。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/e38c54d4ea938e966a3229fdbf714fc568f301fdf4269c57480b74b3fb22800f.png)
另外就是规则的泛化能力弱，甚至同一服务的不同版本、指标的规律性和阈值都可能存在着些许差异。例如下面的例子，两个指标虽然整体上有些相似的波动性，但是它们的取值范围、以及局部的抖动情况都存在着差异，因此需要分别去做监控。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/4cb8b1b0dabd27c82749488a63b1f83c9139a16ec8cb210a54d9671274c27564.png)
基于上述的痛点，我们可以使用智能巡检来解决，其基本思路是：

- 采用无监督的学习算法，自动识别实体的数据特征，构建时序画像。
- 然后根据数据的特征选取不同的算法组合，针对数据流实时建模，完成异常检测任务。
- 检测到异常后，会发送告警通知给用户，然后用户可以打标进行反馈，标记确认或者误报。
- 然后会根据用户的反馈信息训练监督模型，不断优化算法，从而提高检测的准确率。

![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/4fdc068a9e3d0fad4b02603348ef36a46a98f0bc112445703b584f8e6e548451.png)
例如下面的例子，当网络流量骤降的时候，智能巡检会发现当前的值与历史趋势相差较大，因此会触发告警。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/e805b59c807fa4c5d0d2577102fa59bd2066099c2804fa7f5cf1e2ef577cf3bb.png)
### 智能降噪
除了通过算法介入到监控系统，提高监控的准确度，还可以将算法的能力融入到告警管理的流程中，对告警进行智能的降噪管理，从而减少通知的打扰，降低告警通知风暴的可能性。
告警降噪本质上就是将相同的或相似的告警进行合并处理，在确保通知准确可靠的基础上，尽可能减少通知的数量。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/5b1aa975a0199cae2cacbb4b55c54be04d0c05adfc1434e8d3fae8abfda92efe.png)
通过算法的能力，我们可以对告警本身进行预处理，然后通过 MinJoin 聚类算法和向量相似度算法分别处理得到结果，将这两种算法的结果再进行合并，从而得到最终的一个聚类结果。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/0b8c02cdc9923d83791c8929363c0b9269bdfc6f83fbb027f8e88fb3cdd1254e.png)
### 多源告警统一管理
除了将可观测数据进行统一的储存与管理之外，对于许多线下已有的系统，如果希望复用云上的告警管理系统，还可以直接将线下的 Prometheus、Grafana、Zabbix 等数据直接接入到告警管理系统中，从而可以实现全局的告警管理。包括统一的降噪、抑制静默、通知分派等流程。
![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/e1e02d895d6ac7edce3eacac8700580d78af0e88cafb65e99dfed4891412eecf.png)
## 总结和展望
对可观测数据的管理和使用，我们始终认为，只有打破数据的边界，才能够挖掘更多的数据价值。具体来说就是：

- 首先就是就是统一。打破数据之间的边界和壁垒，从采集到存储、查询分析，以及可视化、监控等应用场景，都能够相对统一地来使用这些数据，这样才能将可观测数据的使用门槛尽可能降低，这也是我们一直以来致力于去推动的事情。
- 另一方面就是开放。数据的统一就意味着需要能够连接足够多的生态，无论是数据的流入，还是对上层使用场景的支持。因此开放性与统一性是一个相辅相成的关系。
- 还有一点就是智能。随着数据规模的不断增长，无论是对数据潜在价值的挖掘，还是对数据质量的治理上，单纯的人力不可避免地会存在一些疏漏，因此在这个过程中引入AI算法，帮助我们去更好地理解数据、分析数据、使用数据，一定是未来的大势所趋。

![image.png](/img/src/technical/DevOps下可观测数据质量建设最佳实践/7da8b27a8f8a27a3ce4e7c667c0ca38b49d5a65a547fce3a06a254e14ad4df0b.png)
